{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mugalan/llama-index/blob/main/LLM_Indexing_Storage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dependencies"
      ],
      "metadata": {
        "id": "lG9iqlv2W_QT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pandas import to_datetime\n",
        "#import plotly.figure_factory as ff\n",
        "import plotly.graph_objects as go\n",
        "import json\n",
        "import plotly.express as px\n",
        "import requests\n",
        "import copy\n",
        "import urllib\n",
        "import pathlib\n",
        "import os\n",
        "from IPython.display import HTML, display\n",
        "from IPython.core.display import HTML"
      ],
      "metadata": {
        "id": "hcgJ2e_ltoiT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-core #llama-parse llama-index-readers-file"
      ],
      "metadata": {
        "id": "IGHRvWXXJn9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-openai llama-index-embeddings-openai"
      ],
      "metadata": {
        "id": "ZOPVdJ7GJ6it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import VectorStoreIndex, Document, StorageContext, ServiceContext, load_index_from_storage\n",
        "import llama_index.core.settings as settings\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "OPENAI_API_KEY=\"api-key\"\n",
        "openai_llm = OpenAI(model=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "d6DAV2dTJygZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kpE3sZX6eVdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Orise-Webservice Client"
      ],
      "metadata": {
        "id": "z9wj6YW2WQZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_token_omdAPI(tokenurl,credentials):\n",
        "    data = {\n",
        "        \"username\": credentials['username'],\n",
        "        \"password\": credentials['password']\n",
        "    }\n",
        "    token = requests.post(tokenurl, json=data)\n",
        "    return token\n",
        "\n",
        "def call_omdAPI(webserviceurl,token,apimethod,apidata):\n",
        "    postdata={\"method\":apimethod,\"data\":apidata}\n",
        "    headers = {\"Authorization\": f\"Token {token}\"}\n",
        "    response = requests.post(webserviceurl, json=postdata, headers=headers)\n",
        "    return response"
      ],
      "metadata": {
        "id": "Lb_r8MKb_Pts"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##URL Specifications"
      ],
      "metadata": {
        "id": "w9pV3-uLq9r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "domain=\"coco-green.orise.lk\"\n",
        "subdomain=\"beta-v2/app\"\n",
        "tokenurl=f'https://{domain}/{subdomain}/api/api-token-auth/'\n",
        "webserviceurl=f'https://{domain}/{subdomain}/api/'\n",
        "ai_api_webserviceurl=f'https://{domain}/{subdomain}/api/ai-query/'\n",
        "credentials={\"username\":\"ai-user\",\"password\":\"SYSCORE-demo#1234\"}"
      ],
      "metadata": {
        "id": "kie1Ic7JfOfD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=get_token_omdAPI(tokenurl,credentials)\n",
        "token=response.json()['token']"
      ],
      "metadata": {
        "id": "MeM8ux4YqVLm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Data Manipulation"
      ],
      "metadata": {
        "id": "-00so2jwFDC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apimethod = 'get_all_models'\n",
        "apidata = {}\n",
        "response = call_omdAPI(webserviceurl, token, apimethod, apidata)\n",
        "print(response.status_code)\n",
        "model_records=json.loads(response.json()['response']['data'])['records']\n",
        "df=pd.DataFrame(model_records)\n",
        "df"
      ],
      "metadata": {
        "id": "VFprj-qu0Hj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LlamaIndex"
      ],
      "metadata": {
        "id": "xxr__O_dw3i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import llama_index"
      ],
      "metadata": {
        "id": "3VivafkJvcUl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.vector_stores import SimpleVectorStore\n",
        "from llama_index.core.graph_stores.simple import SimpleGraphStore\n",
        "from llama_index.core import Settings"
      ],
      "metadata": {
        "id": "jwhFOvrRrDv3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Set STORAGE_DIR to the base project directory\n",
        "STORAGE_DIR = \"/content/storage/models\"\n",
        "VECTOR_STORE_DIR = \"/content/storage/models\"\n",
        "\n",
        "def initialize_index(llm, embed_model):\n",
        "    \"\"\"Initialize the vector index and store it persistently.\"\"\"\n",
        "    pathlib.Path(STORAGE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "    pathlib.Path(VECTOR_STORE_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    fake_docstore_doc = {\n",
        "        \"text\": \"This is a placeholder document.\",\n",
        "        \"metadata\": {\"data_id\": \"placeholder-id\"},\n",
        "    }\n",
        "    fake_docstore = {\"doc_id_1\": fake_docstore_doc}\n",
        "\n",
        "    # ✅ Write docstore and index store\n",
        "    with open(os.path.join(STORAGE_DIR, \"docstore.json\"), \"w\") as f:\n",
        "        json.dump(fake_docstore, f)\n",
        "\n",
        "    with open(os.path.join(STORAGE_DIR, \"index_store.json\"), \"w\") as f:\n",
        "        json.dump(fake_docstore, f)\n",
        "\n",
        "    documents = [Document(text=json.dumps(fake_docstore_doc), metadata=fake_docstore_doc)]\n",
        "\n",
        "    vector_store = SimpleVectorStore(persist_dir=VECTOR_STORE_DIR)\n",
        "    StorageContext._default_persist_dir = STORAGE_DIR\n",
        "    # ✅ Create storage context\n",
        "    storage_context = StorageContext.from_defaults(\n",
        "        persist_dir=STORAGE_DIR,\n",
        "        vector_store=vector_store\n",
        "    )\n",
        "\n",
        "    # ✅ Create and persist vector index\n",
        "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context, llm=llm, embed_model=embed_model)\n",
        "    index.storage_context.persist(persist_dir=STORAGE_DIR)\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_records_to_index(llm, embed_model, records):\n",
        "    \"\"\"Add multiple records to the index.\"\"\"\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=STORAGE_DIR)\n",
        "    index = load_index_from_storage(storage_context, llm=llm, embed_model=embed_model)\n",
        "\n",
        "    for record in records:\n",
        "        text = json.dumps(record)\n",
        "        metadata = record\n",
        "        new_document = Document(text=text, metadata=metadata)\n",
        "        index.insert(new_document)  # Insert document into index\n",
        "\n",
        "    index.storage_context.persist(persist_dir=STORAGE_DIR)\n",
        "    return index\n",
        "\n",
        "\n",
        "def add_json_to_index(llm, embed_model, text, metadata):\n",
        "    \"\"\"Add a single JSON document to the index.\"\"\"\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=STORAGE_DIR)\n",
        "    index = load_index_from_storage(storage_context, llm=llm, embed_model=embed_model)\n",
        "\n",
        "    new_document = Document(text=text, metadata=metadata)\n",
        "    index.insert(new_document)\n",
        "    index.storage_context.persist(persist_dir=STORAGE_DIR)\n",
        "    return index\n",
        "\n",
        "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
        "\n",
        "def query_index(query, llm, embed_model, retriever_kwargs={\"top_k\": 1}):\n",
        "    \"\"\"Query the stored index.\"\"\"\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=STORAGE_DIR)\n",
        "\n",
        "    # ✅ Try to load index safely\n",
        "    try:\n",
        "        index = load_index_from_storage(storage_context, llm=llm, embed_model=embed_model)\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Failed to load index: {str(e)}\"}\n",
        "\n",
        "    # ✅ Create a query engine\n",
        "    query_engine = index.as_query_engine(llm=llm, retriever_kwargs=retriever_kwargs)\n",
        "\n",
        "    # ✅ Run query\n",
        "    metadata={}\n",
        "    response = query_engine.query(query)\n",
        "\n",
        "    if hasattr(response, \"metadata\") and isinstance(response.metadata, dict):\n",
        "        first_key = next(iter(response.metadata), None)  # Get the first key\n",
        "        if first_key:\n",
        "            metadata=response.metadata[first_key]\n",
        "\n",
        "\n",
        "    return {\"meta_data\":metadata}"
      ],
      "metadata": {
        "id": "CoJ2Lsdx1SkN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a model index"
      ],
      "metadata": {
        "id": "yp_JHbUY4w7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=initialize_index(openai_llm, embed_model)"
      ],
      "metadata": {
        "id": "CXdWgGN64zrh",
        "outputId": "dcdc638b-6fd2-4b7b-a6d5-d18bcc29663e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.core.graph_stores.simple:No existing llama_index.core.graph_stores.simple found at /content/storage/models/graph_store.json. Initializing a new graph_store from scratch. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "add_records_to_index(openai_llm, embed_model, model_records)"
      ],
      "metadata": {
        "id": "sV-vFC-8HkK5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8a54719f-cbb8-43ab-ac91-08bba78934af"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7a88e71678d0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message=\"RegionSlotAllocation wise for week 24 to 38 for 2025\" #\"A Pie chart of region wise slots allocated\""
      ],
      "metadata": {
        "id": "Hl6idm3lLjUN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_response = query_index(message, openai_llm, embed_model)"
      ],
      "metadata": {
        "id": "SFBRC7OV8XM5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_response"
      ],
      "metadata": {
        "id": "zeSImRnV_yAf",
        "outputId": "280a111f-ced0-4b81-9906-aa58f921549a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'meta_data': {'app_label': 'modelsApp',\n",
              "  'model_name': 'MaxSlotAllocation',\n",
              "  'fields': ['year', 'week', 'product_type', 'slots'],\n",
              "  'description': 'Model MaxSlotAllocation fields are year, week, product_type, slots'}}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_response"
      ],
      "metadata": {
        "id": "TPD0M7kE8c38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.text"
      ],
      "metadata": {
        "id": "edXSYdg8FzjG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}